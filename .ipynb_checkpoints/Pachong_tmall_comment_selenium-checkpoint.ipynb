{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time,re,random,sys\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup \n",
    "#import requests\n",
    "#from fake_useragent import UserAgent\n",
    "#import urllib\n",
    "# import tools\n",
    "from selenium.common.exceptions import NoSuchElementException,TimeoutException,WebDriverException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_comments_infos_fromurlist(path):\n",
    "    \n",
    "    def get_comments1(ps):    #tmall\n",
    "        comments=[]\n",
    "        for comment in re.findall('<div class=\"tm-rate-content\"><div class=\"tm-rate-fulltxt\">(.*?)</div>',ps):\n",
    "            if 'span' not in comment:\n",
    "                comments.append(comment)\n",
    "        return pd.DataFrame({\n",
    "                'comment':comments\n",
    "        })\n",
    "    \n",
    "    def get_comments2(ps):     #taobao\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        a = [i.text.strip() for i in soup.findAll('div',{'class':'J_KgRate_ReviewContent tb-tbcr-content '})]\n",
    "        comments = []\n",
    "        for j,k in enumerate(a):\n",
    "            if '[追加评论]' in k:\n",
    "                continue\n",
    "            if j != len(a)-1:\n",
    "                if '评价方未及时' in k or '此用户没有填写评价' in k:\n",
    "                    if '评价方未及时' not in a[j+1] and '此用户没有填写评价' not in a[j+1]:\n",
    "                        comments.append(a[j+1].strip('[追加评论]\\n          \\n          '))\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    comments.append(k)\n",
    "            else:\n",
    "                comments.append(k)\n",
    "        return pd.DataFrame({\n",
    "                'comment':comments\n",
    "        })\n",
    "        \n",
    "    \n",
    "    def tmallmethod(url):\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            WebDriverWait(driver, timeout).until(\n",
    "                EC.presence_of_element_located((By.ID, \"J_TabBarBox\"))  # 判断页面是否初步加载成功的标记\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            print('宝贝链接未加载成功')\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        title = soup.findAll(name='h1',attrs={'data-spm':\"1000983\"})[0].text.strip()\n",
    "\n",
    "        try:\n",
    "            # 页面上拉600，看到TabBarBox\n",
    "            js = \"window.scrollTo(0,600)\"\n",
    "            driver.execute_script(js)\n",
    "        except WebDriverException:\n",
    "            print('上拉寻找评论区时出现问题')\n",
    "\n",
    "        time.sleep(random.uniform(1,2))\n",
    "        # 点击累计评论TAB\n",
    "        driver.find_element_by_xpath('//a[@href=\"#J_Reviews\"]').click()\n",
    "        time.sleep(random.uniform(1,2))\n",
    "        print('成功点击累计评论TAB')\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                # rate-grid的正则表达式\n",
    "                driver.find_element_by_xpath('//*[@id=\"J_Reviews\"]/div/div[6]')\n",
    "                print('已经成功加载到评论区信息')\n",
    "                break\n",
    "            except NoSuchElementException:\n",
    "                print('评论信息中元素还未加载')\n",
    "                continue\n",
    "\n",
    "\n",
    "        allcmts = pd.DataFrame()\n",
    "\n",
    "        count = 2\n",
    "        while True:\n",
    "\n",
    "            cmts = get_comments1(driver.page_source)\n",
    "            allcmts = pd.concat([allcmts,cmts],axis=0,ignore_index=True)\n",
    "\n",
    "            try:\n",
    "                js = \"window.scrollTo(0,2500)\"\n",
    "                driver.execute_script(js)\n",
    "                time.sleep(random.uniform(1,2))\n",
    "                # 点击下一页\n",
    "                nextpage = driver.find_element_by_css_selector('#J_Reviews > div > div.rate-page > div > a:last-child')\n",
    "                driver.execute_script('arguments[0].click()',nextpage)\n",
    "                print('\\r已成功点击第' + str(count) +'页',end='')\n",
    "\n",
    "            except NoSuchElementException:\n",
    "                print('找不到翻页按钮,结束查询--(%s)' % title)\n",
    "                break\n",
    "\n",
    "            count += 1\n",
    "            time.sleep(random.uniform(1,2))\n",
    "\n",
    "        today = str(time.strftime('%Y-%m-%d',time.localtime(time.time())))\n",
    "        allcmts.to_csv('%s-%s.csv'%(today,title),index=False,header=False,encoding='utf_8_sig')\n",
    "        \n",
    "    def taobaomethod(url):\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            WebDriverWait(driver, timeout).until(\n",
    "                EC.presence_of_element_located((By.ID, \"J_TabBar\"))  # 判断页面是否初步加载成功的标记\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            print('宝贝链接未加载成功')\n",
    "        \n",
    "        time.sleep(3)\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        title = [i.find('h3')['data-title'] for i in soup.findAll(name='div',attrs={'class':\"tb-title\"})][0].strip()\n",
    "\n",
    "        try:\n",
    "            # 页面上拉600，看到TabBarBox\n",
    "            js = \"window.scrollTo(0,1000)\"\n",
    "            driver.execute_script(js)\n",
    "        except WebDriverException:\n",
    "            print('上拉寻找评论区时出现问题')\n",
    "\n",
    "        time.sleep(random.uniform(1,2))\n",
    "        # 点击累计评论TAB\n",
    "        driver.find_element_by_xpath('//ul[@id=\"J_TabBar\"]/li[2]').click()\n",
    "        print('成功点击累计评论TAB')\n",
    "        time.sleep(random.uniform(2,3))\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                # rate-grid的正则表达式\n",
    "                driver.find_element_by_xpath('//div[@class=\"sub-wrap\"]')\n",
    "                print('已经成功加载到评论区信息')\n",
    "                break\n",
    "            except NoSuchElementException:\n",
    "                print('评论信息中元素还未加载')\n",
    "                continue\n",
    "\n",
    "        allcmts = pd.DataFrame()\n",
    "\n",
    "        count = 2\n",
    "        while count<100:\n",
    "\n",
    "            cmts = get_comments2(driver.page_source)\n",
    "            allcmts = pd.concat([allcmts,cmts],axis=0,ignore_index=True)\n",
    "\n",
    "            try:\n",
    "                js = \"window.scrollTo(0,4000)\"\n",
    "                driver.execute_script(js)\n",
    "                time.sleep(random.uniform(1,2))\n",
    "                # 点击下一页\n",
    "                nextpage = driver.find_element_by_xpath('//li[@class=\"pg-next\"]')\n",
    "                driver.execute_script('arguments[0].click()',nextpage)\n",
    "                print('\\r已成功点击第' + str(count) +'页',end='')\n",
    "\n",
    "            except NoSuchElementException:\n",
    "                print('找不到翻页按钮,结束查询--(%s)' % title)\n",
    "                break\n",
    "\n",
    "            count += 1\n",
    "            time.sleep(random.uniform(1,2))\n",
    "\n",
    "        today = str(time.strftime('%Y-%m-%d',time.localtime(time.time())))\n",
    "        allcmts.to_csv('%s-%s.csv'%(today,title),index=False,header=False,encoding='utf_8_sig')\n",
    "            \n",
    "    set = pd.read_csv(path,header=None) \n",
    "    urlist = [ i[0] for i in set.values.tolist()]\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-automation']) \n",
    "    options.add_experimental_option(\"prefs\", {\"profile.managed_default_content_settings.images\": 2})\n",
    "    driver = webdriver.Chrome(options = options)\n",
    "    driver.set_window_size(1000,700)\n",
    "    timeout = 20\n",
    "        \n",
    "    for url in urlist:\n",
    "        if not url.startswith('http'):\n",
    "            url = 'https:' + url\n",
    "        print('开始寻找评论from:' + url)\n",
    "        if url.startswith('https://item.taobao.com/'):\n",
    "            taobaomethod(url)\n",
    "        elif url.startswith('https://detail.tmall.com/'):\n",
    "            tmallmethod(url)\n",
    "        else:\n",
    "            print('网址错误%s'%url)\n",
    "            break\n",
    "        \n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始寻找评论from:https://item.taobao.com/item.htm?spm=a230r.1.14.239.595557e1yiIh9G&id=577539847710&ns=1&abbucket=15#detail\n"
     ]
    },
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-72dd108b1261>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdeal_comments_infos_fromurlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'123.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-d4e470e6ca56>\u001b[0m in \u001b[0;36mdeal_comments_infos_fromurlist\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'开始寻找评论from:'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://item.taobao.com/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             \u001b[0mtaobaomethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://detail.tmall.com/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m             \u001b[0mtmallmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-d4e470e6ca56>\u001b[0m in \u001b[0;36mtaobaomethod\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'宝贝链接未加载成功'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'h3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data-title'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"tb-title\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\miniconda3\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[1;34m\"Couldn't find a tree builder with the features you \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                     \u001b[1;34m\"requested: %s. Do you need to install a parser library?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                     % \",\".join(features))\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[0mbuilder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuilder_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m             if not (original_features == builder.NAME or\n",
      "\u001b[1;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "deal_comments_infos_fromurlist('123.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确认你的网页池\n",
    "set = pd.read_csv(path,header=None) \n",
    "urlist = [ i[0] for i in set.values.tolist()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
